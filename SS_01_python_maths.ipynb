{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckraju/fmml-jan/blob/main/SS_01_python_maths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Linear Algebra"
      ],
      "metadata": {
        "id": "49V2aI9V_6hX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_hrogC6cmMu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System of Linear Equations...\n",
        "\n",
        "- Systems of linear equations play a **central part** of linear algebra. Many problems can be formulated as systems of linear equations, and linear algebra gives us the tools for solving them.\n",
        "- For intuitive explanation and proper visualization, we'll consider 2-D plane and system of two linear equations with two\n",
        "variables which can be geometrically interpreted as the intersection of two lines. Every linear equation represents a line. <center><img src=\"https://drive.google.com/uc?export=view&id=11XSuesqkAEWgiBD7ZEj2VdfBIzoOQkYI\" alt=\"lin_eq\" height=\"350\"></center>\n",
        "\n",
        "- Let's solve this equation:\n",
        "\\begin{equation}4x_1 + 4x_2 = 5\\\\ 2x_1 - 4x_2 = 1\\end{equation}\n",
        "- On solving: one will get $x_1 = 1,\\ x_2 = 1/4$\n",
        "- Can we do better?\n",
        "- For a systematic approach to solving systems of linear equations, we will introduce a useful compact notation.\n",
        "\\begin{equation}x_1\\left[\\begin{array}{c}\n",
        "4 \\\\\n",
        "2\n",
        "\\end{array}\\right]+x_2\\left[\\begin{array}{c}\n",
        "4 \\\\\n",
        "-4\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "5 \\\\\n",
        "1\n",
        "\\end{array}\\right]\\end{equation}\n",
        "\n",
        "- In general it can be expressed as:\n",
        "\\begin{equation}x_1\\left[\\begin{array}{c}\n",
        "a_{11} \\\\\n",
        "\\vdots \\\\\n",
        "a_{m 1}\n",
        "\\end{array}\\right]+x_2\\left[\\begin{array}{c}\n",
        "a_{12} \\\\\n",
        "\\vdots \\\\\n",
        "a_{m 2}\n",
        "\\end{array}\\right]+\\cdots+x_n\\left[\\begin{array}{c}\n",
        "a_{1 n} \\\\\n",
        "\\vdots \\\\\n",
        "a_{m n}\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "b_1 \\\\\n",
        "\\vdots \\\\\n",
        "b_m\n",
        "\\end{array}\\right]\\end{equation}\n",
        "\n",
        "- Now going one level more and compacting above representation will generate:\n",
        "\\begin{equation}\\left[\\begin{array}{ccc}\n",
        "a_{11} & \\cdots & a_{1 n} \\\\\n",
        "\\vdots & & \\vdots \\\\\n",
        "a_{m 1} & \\cdots & a_{m n}\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "x_1 \\\\\n",
        "\\vdots \\\\\n",
        "x_n\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "b_1 \\\\\n",
        "\\vdots \\\\\n",
        "b_m\n",
        "\\end{array}\\right]\\end{equation}\n",
        "\n",
        "- Matrices play a central role in linear algebra. They can be used to compactly represent systems of linear equations, but they also represent linear functions (linear mappings) as we will see later.\n",
        "\n",
        "- **Definition (Matrix)**. With $m, n \\in \\mathbb{N}$ a real-valued $(m, n)$ matrix $\\boldsymbol{A}$ is an $m \\cdot n$-tuple of elements $a_{i j}, i=1, \\ldots, m, j=1, \\ldots, n$, which is ordered according to a rectangular scheme consisting of $m$ rows and $n$ columns:\n",
        "\\begin{equation}\\boldsymbol{A}=\\left[\\begin{array}{cccc}\n",
        "a_{11} & a_{12} & \\cdots & a_{1 n} \\\\\n",
        "a_{21} & a_{22} & \\cdots & a_{2 n} \\\\\n",
        "\\vdots & \\vdots & & \\vdots \\\\\n",
        "a_{m 1} & a_{m 2} & \\cdots & a_{m n}\n",
        "\\end{array}\\right], \\quad a_{i j} \\in \\mathbb{R}\\end{equation}\n",
        "\n",
        "- By convention $(1, n)$-matrices are called rows and $(m, 1)$-matrices are called columns. These special matrices are also called row/column vectors.\n"
      ],
      "metadata": {
        "id": "MiF41lNn__8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Vectors**: Usual notation we've seen for vectors in high-school: $\\vec{x}$ and $\\vec{y}$.\n",
        "- They can be added ($\\vec{x} + \\vec{y} = \\vec{z}$) or multiplied by a scalar ($\\lambda\\vec{x}$).\n",
        "\n",
        "- Matrix **Addition**: The sum of two matrices $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}, \\boldsymbol{B} \\in \\mathbb{R}^{m \\times n}$ is defined as the elementwise sum, i.e.,\n",
        "\\begin{equation}\\boldsymbol{A}+\\boldsymbol{B}:=\\left[\\begin{array}{ccc}\n",
        "a_{11}+b_{11} & \\cdots & a_{1 n}+b_{1 n} \\\\\n",
        "\\vdots & & \\vdots \\\\\n",
        "a_{m 1}+b_{m 1} & \\cdots & a_{m n}+b_{m n}\n",
        "\\end{array}\\right] \\in \\mathbb{R}^{m \\times n}\\end{equation}\n",
        "\n",
        "- Matrix **Multiplication**: For matrices $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}, \\boldsymbol{B} \\in \\mathbb{R}^{n \\times k}$, the elements $c_{i j}$ of the product $\\boldsymbol{C}=\\boldsymbol{A} \\boldsymbol{B} \\in \\mathbb{R}^{m \\times k}$ are computed as:\n",
        "\\begin{equation}c_{i j}=\\sum_{l=1}^n a_{i l} b_{l j}, \\quad i=1, \\ldots, m, \\quad j=1, \\ldots, k\\end{equation}\n",
        "\n",
        "- With above example, this can be formulated as:\n",
        "\\begin{equation}\\left[\\begin{array}{cc}\n",
        "4 & 4 \\\\\n",
        "2 & -4\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "x_1 \\\\\n",
        "x_2\n",
        "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
        "5 \\\\\n",
        "1\n",
        "\\end{array}\\right]\\end{equation}\n",
        "\n",
        "- Let's consider ONE more example:\n",
        "\\begin{equation}\\begin{aligned}\n",
        "& 2 x_1+3 x_2+5 x_3=1 \\\\\n",
        "& 4 x_1-2 x_2-7 x_3=8 \\\\\n",
        "& 9 x_1+5 x_2-3 x_3=2\n",
        "\\end{aligned}\\end{equation}\n",
        "And its compact form representation?\n",
        "\n",
        "\\begin{equation}\\left[\\begin{array}{ccc}\n",
        "2 & 3 & 5 \\\\\n",
        "4 & -2 & -7 \\\\\n",
        "9 & 5 & -3\n",
        "\\end{array}\\right]\\left[\\begin{array}{l}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "x_3\n",
        "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
        "1 \\\\\n",
        "8 \\\\\n",
        "2\n",
        "\\end{array}\\right]\\end{equation} Note that $x_1$ scales the first column, $x_2$ the second one, and $x_3$ the third one.\n",
        "\n",
        "- Generally, a system of linear equations can be compactly represented in their matrix form as $\\boldsymbol{A x}=\\boldsymbol{b}$, and the product $\\boldsymbol{A} \\boldsymbol{x}$ is a (linear) combination of the columns of $\\boldsymbol{A}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "3WOKs3LHosfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add two matrix and Multiply scalar to Matrix.\n",
        "A = np.array([[2,3,5],[4,-2,-7],[9,5,-3]])\n",
        "B = np.array([[1,7,2],[-1,2,5],[6,5,4]])\n",
        "print(f\"A = \\n{A}\")\n",
        "print(f\"B = \\n{B}\")\n",
        "\n",
        "A_plus_B = A + B\n",
        "print(f\"A + B = \\n{A_plus_B}\")\n",
        "print(f\"4*A = \\n{4*A}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpvhN0nfrGrE",
        "outputId": "ac073956-0e2a-4e69-dd6b-c21d7e5beb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = \n",
            "[[ 2  3  5]\n",
            " [ 4 -2 -7]\n",
            " [ 9  5 -3]]\n",
            "B = \n",
            "[[ 1  7  2]\n",
            " [-1  2  5]\n",
            " [ 6  5  4]]\n",
            "A + B = \n",
            "[[ 3 10  7]\n",
            " [ 3  0 -2]\n",
            " [15 10  1]]\n",
            "4*A = \n",
            "[[  8  12  20]\n",
            " [ 16  -8 -28]\n",
            " [ 36  20 -12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Inverse and Transpose\n",
        "\n",
        "### Inverse\n",
        "- The above form relates to the equation: $ax = b \\implies x = b/a$.\n",
        "- Can we do the same thing here? $\\boldsymbol{x}=\\boldsymbol{b}/\\boldsymbol{A}$. But this is not a correct operation, as a vector couldn't be divided by a matrix (sounds weird right?).\n",
        "- Hence, we need something in place of $1/\\boldsymbol{A}$. And since $\\boldsymbol{x}$ is vector, for sure $1/\\boldsymbol{A}$ should be a matrix.\n",
        "- As thus the notation $\\boldsymbol{A}^{-1}:=$ that represents $1/\\boldsymbol{A}$ arise.\n",
        "- As multipying $a$ with $1/a$ (i.e., $a\\dfrac{1}{a} = 1$) gives us $1$. Similarly, $\\boldsymbol{A}\\boldsymbol{A}^{-1} = \\boldsymbol{I} = \\boldsymbol{A}^{-1}\\boldsymbol{A}$, where $\\boldsymbol{I}$ is the identity matrix. $$\\boldsymbol{I} = \\left[\\begin{array}{cccccc}\n",
        "1 & 0 & \\cdot & . & \\cdot & 0 \\\\\n",
        "0 & 1 & \\cdot & \\cdot & \\cdot & 0 \\\\\n",
        "\\cdot & \\cdot & 1 & & & \\cdot \\\\\n",
        "\\cdot & \\cdot & & 1 & & \\cdot \\\\\n",
        ". & \\cdot & & & 1 & \\cdot \\\\\n",
        "0 & 0 & \\cdot & \\cdot & \\cdot & 1\n",
        "\\end{array}\\right]_{n \\times n}$$\n",
        "\n",
        "- Now how to compute inverses? <img src=\"https://em-content.zobj.net/source/animated-noto-color-emoji/356/thinking-face_1f914.gif\" alt=\"curious-emoji\" width=\"50\">\n",
        "\n",
        "- There are a lot of techniques, namely:\n",
        "    1. Popular one: **Row-echelon form** ([Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination))\n",
        "    2. **Matrix of Minors and Cofactors**: E.g.,\n",
        "$$\n",
        "\\boldsymbol{A}^{-1}=\\frac{1}{a_{11} a_{22}-a_{12} a_{21}}\\left[\\begin{array}{cc}\n",
        "a_{22} & -a_{12} \\\\\n",
        "-a_{21} & a_{11}\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "    3. **LU Decomposition**: [Link](https://en.wikipedia.org/wiki/LU_decomposition)\n",
        "    4. [**Eigenvalue Decomposition**](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) (if matrix is diagonalizable)\n",
        "    5. [**Singular Value Decomposition**](https://en.wikipedia.org/wiki/Singular_value_decomposition) (usually followed by machines for inverse computation)."
      ],
      "metadata": {
        "id": "M0EBe2CEtbAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solving system of linear equations as mentioned above:\n",
        "# Example 1:\n",
        "A = np.array([[4, 4], [2, -4]])\n",
        "b = np.array([[5], [1]])\n",
        "# x = A^-1 * b\n",
        "x = np.linalg.inv(A) @ b\n",
        "print(f\"Vector x = \\n{x}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW6J_xXvF41h",
        "outputId": "731f4bf3-21c0-446d-da4a-91a2d6665a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector x = \n",
            "[[1.  ]\n",
            " [0.25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second problem\n",
        "A = np.array([[2,3,5], [4,-2,-7], [9,5,-3]])\n",
        "b = np.array([[1], [8], [2]])\n",
        "x = np.linalg.inv(A) @ b\n",
        "print(f\"Vector x = \\n{x}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXU42FSbG9CU",
        "outputId": "31555f9e-f4a4-4ac8-f94e-b00dc66e0220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector x = \n",
            "[[ 2.44537815]\n",
            " [-3.28571429]\n",
            " [ 1.19327731]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transpose\n",
        "\n",
        "- **Definition (Transpose)**. For $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ the matrix $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times m}$ with $b_{i j}=a_{j i}$ is called the transpose of $\\boldsymbol{A}$. We write $\\boldsymbol{B}=\\boldsymbol{A}^{\\top}$.\n",
        "\\begin{equation}\\boldsymbol{A} = \\left[\\begin{array}{rrr}\n",
        "1 & 5 & 9 \\\\\n",
        "2 & 6 & 10 \\\\\n",
        "3 & 7 & 11 \\\\\n",
        "4 & 8 & 12\n",
        "\\end{array}\\right],\\ \\boldsymbol{A}^T = \\left[\\begin{array}{rrrr}\n",
        "1 & 2 & 3 & 4 \\\\\n",
        "5 & 6 & 7 & 8 \\\\\n",
        "9 & 10 & 11 & 12\n",
        "\\end{array}\\right]\\end{equation}\n",
        "\n",
        "- **Definition (Symmetric Matrix)**. A matrix $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$ is symmetric if $\\boldsymbol{A}=\\boldsymbol{A}^{\\top}$\n",
        "- **Definition (Skew-Symmetric Matrix)**. $\\boldsymbol{A}=-\\boldsymbol{A}^{\\top}$\n",
        "$$\\boldsymbol{A}_{\\text{Symm}} = \\left[\\begin{array}{ccc}\n",
        "1 & 1 & -1 \\\\\n",
        "1 & 2 & 0 \\\\\n",
        "-1 & 0 & 5\n",
        "\\end{array}\\right],\\ \\boldsymbol{A}_{\\text{SkewSymm}} = \\left[\\begin{array}{ccc}\n",
        "0 & 1 & -2 \\\\\n",
        "-1 & 0 & 3 \\\\\n",
        "2 & -3 & 0\n",
        "\\end{array}\\right]$$"
      ],
      "metadata": {
        "id": "hoIgsLVfwZg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tranposing Matrix and Vectors\n",
        "A = np.array([[1,5,9], [2,6,10], [3,7,11], [4,8,12]])\n",
        "print(f\"A = \\n{A}\\n and A^T = \\n{A.T}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLtVPzPGHYVI",
        "outputId": "85fcc62f-d6f6-4331-8b0d-59fd1e6e2b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = \n",
            "[[ 1  5  9]\n",
            " [ 2  6 10]\n",
            " [ 3  7 11]\n",
            " [ 4  8 12]]\n",
            " and A^T = \n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  7  8]\n",
            " [ 9 10 11 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix as transfromation (Revisiting what we have started):\n",
        "\n",
        "- Consider a $2\\times 2$ matrix: $\\boldsymbol{A}=\\left[\\begin{array}{cc}\n",
        "3 & 1 \\\\\n",
        "1 & 2\n",
        "\\end{array}\\right]$.\n",
        "\n",
        "- $\\boldsymbol{A}\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
        "3 & 1 \\\\\n",
        "1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
        "3 \\\\\n",
        "1\n",
        "\\end{array}\\right]$, and $\\left[\\begin{array}{cc}\n",
        "3 & 1 \\\\\n",
        "1 & 2\n",
        "\\end{array}\\right]\\left[\\begin{array}{c}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]$\n",
        "\n",
        "- $\\boldsymbol{A}$ sends $\\left[\\begin{array}{c}\n",
        "1 \\\\0\n",
        "\\end{array}\\right]$ (x-axis) $ ⟶ \\left[\\begin{array}{c}\n",
        "3 \\\\\n",
        "1\n",
        "\\end{array}\\right]$ and $\\left[\\begin{array}{c}\n",
        "0 \\\\1\n",
        "\\end{array}\\right]$ (y-axis) $ ⟶ \\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right]$\n",
        "\n",
        "- $\\boldsymbol{A}\\left[\\begin{array}{c}\n",
        "x_1 \\\\\n",
        "x_2\n",
        "\\end{array}\\right] = \\boldsymbol{A}\\left(x_1\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right] + x_2\\left[\\begin{array}{c}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]\\right) = x_1\\left(\\boldsymbol{A}\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right]\\right) + x_2\\left(\\boldsymbol{A}\\left[\\begin{array}{c}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]\\right) = x_1\\left[\\begin{array}{c}\n",
        "3 \\\\\n",
        "1\n",
        "\\end{array}\\right] + x_2\\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
        "3 x_1 + x_2 \\\\\n",
        "x_1 + 2 x_2\n",
        "\\end{array}\\right]$\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1ZqkP5ZasljmbWL9rsaKBORkWaBD1-xzy\" height=\"300\"></center>"
      ],
      "metadata": {
        "id": "RSePUU1O7eig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Independence (Intuition):\n",
        "\n",
        "- High-school: Any vector in 2D represented as $⟶ x\\hat{i} + y\\hat{j}$, where this vector is a point in 2D plane with coordinates $= (x, y)$ <center><img src=\"https://drive.google.com/uc?export=view&id=1jpTgEjosaEaQ2DWzCSmWkueJOC2IABJm\"></center>\n",
        "\n",
        "- What are these $\\hat{i}$ and $\\hat{j}$?\n",
        "\n",
        "- Informally: x-axis and y-axis. Formally: Unit vectors along x-axis and y-axis (or $\\hat{i} = \\left[\\begin{array}{c}\n",
        "1 \\\\\n",
        "0\n",
        "\\end{array}\\right],\\ \\hat{j} = \\left[\\begin{array}{c}\n",
        "0 \\\\\n",
        "1\n",
        "\\end{array}\\right]$).\n",
        "\n",
        "- ***Can we write $\\hat{j}$ in terms of $\\hat{i}$***? **NEVER**\n",
        "\n",
        "- And this phenomenon is popularly called: ***Linear Independence***. Informally, if none of the vectors from a set of vectors can be expressed/represented in terms of the rest, then we exhibit the property of linear independence.\n",
        "\n",
        "- Questions (are they linearly dependent or independent):\n",
        "    - $4\\hat{i} + 3\\hat{j}$ and $\\hat{i}$\n",
        "    - $4\\hat{i}$ and $\\hat{i}$\n",
        "    - $4\\hat{i} + 3\\hat{j}$ and $\\hat{j}$\n",
        "    - $3\\hat{j}$ and $\\hat{i}+\\hat{j}$\n",
        "    - \\begin{equation}\\left[\\begin{array}{c} 1 \\\\ 2\\end{array}\\right]\\text{and}\\left[\\begin{array}{c} 2 \\\\ 4\\end{array}\\right]\\end{equation}\n",
        "- This independence is essential, as we saw, to represent any vector in this 2D plane (or 2D space!). And this is what we call ***basis vectors*** (basis $←$ base/foundation of 2D space)\n",
        "\n",
        "- Definition: A set of linearly independent vectors that spans the whole space.\n",
        "\n",
        "- Ask yourself a question: *Can two linearly dependent vectors span the 2D space? Take example $4\\hat{i}$ and $\\hat{i}$*\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1YbiQP6hBKELM0jyCwXeXB8BIbzzskfnU\" alt=\"\" height=\"300\"></center>\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "5AqquQTxNJGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inner Product\n",
        "\n",
        "### Dot-product\n",
        "\n",
        "- Revisit to High-school: Given two vectors $\\vec{a}$ and $\\vec{b}$, dot-product between them $ = \\vec{a}.\\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos\\theta \\stackrel{?}{=} a_1.b_1 + a_2.b_2 + \\ldots + a_n.b_n = \\boldsymbol{a}^T\\boldsymbol{b}$.\n",
        "- E.g., $\\left[\\begin{array}{l}\n",
        "2 \\\\\n",
        "7 \\\\\n",
        "1\n",
        "\\end{array}\\right] \\cdot\\left[\\begin{array}{l}\n",
        "8 \\\\\n",
        "2 \\\\\n",
        "8\n",
        "\\end{array}\\right] = 2\\times 8 + 7\\times 2 + 1\\times 8 = 38$\n",
        "\n",
        "- What does that mean? </center> <center><img src=\"https://drive.google.com/uc?export=view&id=1dgweXEDRVmvHjkZBS92qDiVOxKEcD0ij\" width=\"450\"></center>  Falsh a light from top and get the projection of $\\vec{a}$ onto $\\vec{b}$ ($\\|\\vec{a}\\|\\cos\\theta$) followed by multiplying it with $\\|\\vec{b}\\|$. Note: $\\cos\\theta = \\dfrac{\\text{base}}{\\text{hypotenuse}} = \\dfrac{\\text{base}}{\\|\\vec{a}\\|}$; $\\implies \\text{base} = \\|\\vec{a}\\|\\cos\\theta$ <center><img src=\"https://drive.google.com/uc?export=view&id=1MnHDDunb34XL0n2i6OBaC8YlKsSJw3vb\" width=\"300\">\n",
        "\n",
        "- What will happen if one of the vetcor is perpendiclar to another? (e.g., $\\vec{a}\\perp\\vec{b}$)\n",
        "\n",
        "- What will be the dot-product of a vector with itself?\n",
        "$\\vec{a}.\\vec{a} = a_1^2 + a_2^2 + \\ldots + a_n^2 = \\|\\vec{a}\\|\\|\\vec{a}\\|\\cos 0^o = \\|\\vec{a}\\|^2$\n",
        "\n",
        "- [Fun fact]: Did you mark something?\\\n",
        "$\\|\\vec{a}\\|^2 = a_1^2 + a_2^2 + \\ldots + a_n^2$. This is the definition for magnitude of a vector, which intuitively follows *Pythagorean* theorem to give the most famous notion of distance in real world, the ***Euclidean distance***.\n",
        "\n",
        "- *For more intuition and how this $\\|\\vec{a}\\|\\|\\vec{b}\\|\\cos\\theta \\stackrel{?}{=} a_1.b_1 + a_2.b_2 + \\ldots + a_n.b_n$, follow this NICE [video](https://www.youtube.com/watch?v=LyGKycYT2v0)*"
      ],
      "metadata": {
        "id": "aURfurWSdlc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dot Product\n",
        "a = np.array([[2], [7], [1]])\n",
        "b = np.array([[8], [2], [8]])\n",
        "a_dot_b = a.T @ b\n",
        "print(f\"a.b = {a_dot_b}\")\n",
        "\n",
        "# Magnitude of a\n",
        "a_dot_a = a.T @ a\n",
        "print(f\"Magnitude of vector a = {np.sqrt(a_dot_a)}\")\n",
        "print(f\"Magnitude computed using np.linalg.norm for a = {np.linalg.norm(a)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2L5NT1CHwwS",
        "outputId": "f6679fec-bc32-44e9-e1d9-d2bbdf22a2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.b = [[38]]\n",
            "Magnitude of vector a = [[7.34846923]]\n",
            "Magnitude computed using np.linalg.norm for a = 7.3484692283495345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inner Product - $⟨.,.⟩$\n",
        "\n",
        "- What we have just seen now, is one special form of inner product.\n",
        "- In general, it allows the introduction of intuitive geometrical concepts, such as the length of a vector and the angle or distance between two vectors.\n",
        "- To generalize such concepts, we require a definite set of mathemaical properties and a well-defined vector-space (e.g., 2D space). For more info, follow this [link](https://en.wikipedia.org/wiki/Inner_product_space).\n",
        "- For our use case, we will consider the simple definition which we already witnessed:\n",
        "    1. $⟨.,.⟩: V → \\mathbb{R}$, Take two vectors and maps them onto a real number. (E.g., $\\vec{a}.\\vec{b}$ is scalar)\n",
        "    2. $⟨\\vec{a},\\vec{a}⟩\\geq 0$, The vector magnitude is always positive (more correctly, non-negative).\n",
        "    3. $⟨\\vec{a},\\vec{b}⟩ = ⟨\\vec{b},\\vec{a}⟩$, Order doesn't matter (Commutative).\n",
        "\n",
        "- Example: Consider $V = \\mathbb{R}^2$. If we define, $$⟨\\boldsymbol{x}, \\boldsymbol{y}⟩ = x_1y_1 - (x_1y_2 + x_2y_1) + 2 x_2y_2$$, then $⟨.,.⟩$ is an inner product that's different from the dot-product.\n",
        "\n",
        "- What is $\\|\\boldsymbol{x}\\| = \\sqrt{⟨\\boldsymbol{x}, \\boldsymbol{x}⟩}$ with above inner-product rule?\n",
        "\n",
        "- Infact, every such inner-product operation induce its own norm (*the fancy mathematical term for magnitude*). Along with the given vector-space, they together forms, \"*normed linear/vector space*\" to which *Hilbert* space is a subspace.\n",
        "\n",
        "- Next, we'll discuss on norms which are not induced by inner-product (except one). *Feel free to skip it.*\n"
      ],
      "metadata": {
        "id": "D5lZaNQ2u3u5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [OPTIONAL] Norm - The Magnitude Calculator\n",
        "\n",
        "\n",
        "- Who said that adding up squared components is the distance. Since our world is *euclidean* or *cartesian* in nature, squaring things makes sense.\n",
        "\n",
        "- $\\|\\vec{a}\\|^p = a_1^p + a_2^p + \\ldots + a_n^p$ can also be termed as a way of looking at magnitude of $\\vec{a}$, i.e., $\\|\\vec{a}\\|_p = \\left(a_1^p + a_2^p + \\ldots + a_n^p\\right)^{1/p}$. And mathematically, this is termed as $L^p$ norm (or *Minkowski Distance*).\n",
        "\n",
        "- Similarly, *Euclidean* norm is called as $L^2$ norm, while $L^1$ is called as *Manhattan* norm.\n",
        "\n",
        "- Such way of computing distance or magnitude, induce a mathematical space on its own, which we simply called [$L^p$ space](https://en.wikipedia.org/wiki/Lp_space)."
      ],
      "metadata": {
        "id": "rmIAZVPn2seP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Measure\n",
        "\n",
        "- How similar or dissimilar the vectors are can be  gauged through their relative distance in the space.\n",
        "### Distance as Similarity\n",
        "\n",
        "- **Euclidean Distance ($L^2$)**: Length of a segment connecting the two points. The most obvious way of representing distance between two points. Formula $ = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$ <center><img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*I_CIWl4fQfsKHSYk90kgpw.png\" width=\"300\" alt=\"euclidean-dist\"></center>\n",
        "- **Manhattan distance ($L^1$)**: also known as city block distance or L1 distance measures the distance between two points by summing the absolute differences of their Cartesian coordinates. It represents the distance traveled along orthogonal axes in a grid-like pattern and is commonly used in image processing and clustering algorithms. Formula $ = |x_2 - x_1| + |y_2 - y_1|$ <center><img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*zEc7etVDTIjYtIlK3N2-4Q.png\" width=\"300\" alt=\"euclidean-dist\"></center>\n",
        "\n",
        "### Angle as Similarity\n",
        "\n",
        "- **Cosine similarity $\\left(\\cos\\theta = \\dfrac{\\vec{a}.\\vec{b}}{\\|\\vec{a}\\|\\|\\vec{b}\\|}\\right)$**: Cosine similarity measures the cosine of the angle between two vectors. It ranges from -1 to 1, where 1 indicates perfect similarity, 0 indicates no similarity, and -1 indicates perfect dissimilarity. Cosine similarity is widely used in text mining and information retrieval tasks. It is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. <center><img src=\"https://www.oreilly.com/api/v2/epubs/9781788295758/files/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png\" width=\"300\" alt=\"euclidean-dist\"></center>"
      ],
      "metadata": {
        "id": "BkkrKxrv_JJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity measure between two vectors.\n",
        "a = np.array([[2], [7], [1]])\n",
        "b = np.array([[8], [2], [8]])\n",
        "\n",
        "# Euclidean distance.\n",
        "e_ab = np.sqrt(np.sum((b-a)**2))\n",
        "print(f\"Euclidean distance between a and b = {e_ab}\")\n",
        "print(f\"Verify: Same as L2 norm of b-a vector = {np.linalg.norm(b-a)}\")\n",
        "\n",
        "# Manhattan distance.\n",
        "m_ab = np.sum(np.abs(b-a))\n",
        "print(f\"Manhattan distance between a and b = {m_ab}\")\n",
        "print(f\"Verify: Same as L1 norm of b-a vector = {np.linalg.norm(b-a, ord=1)}\")\n",
        "\n",
        "# Cosine similarity.\n",
        "c_ab = (a.T @ b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "print(f\"Cosine Similarity between a and b = {c_ab}\")\n",
        "print(f\"Verify: From sklearn package = {cosine_similarity(a.T, b.T)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L21lY7O3IlXy",
        "outputId": "c816eaa8-ac35-4ced-d8ac-abf995828361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean distance between a and b = 10.488088481701515\n",
            "Verify: Same as L2 norm of b-a vector = 10.488088481701515\n",
            "Manhattan distance between a and b = 18\n",
            "Verify: Same as L1 norm of b-a vector = 18.0\n",
            "Cosine Similarity between a and b = [[0.4500904]]\n",
            "Verify: From sklearn package = [[0.4500904]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-Valued function of $\\boldsymbol{A}$\n",
        "### Determinant of square-matrix  $\\boldsymbol{A}$\n",
        "\n",
        "$$\n",
        "\\operatorname{det}(\\boldsymbol{A}):=\\left|\\begin{array}{cccc}\n",
        "a_{11} & a_{12} & \\ldots & a_{1 n} \\\\\n",
        "a_{21} & a_{22} & \\ldots & a_{2 n} \\\\\n",
        "\\vdots & & \\ddots & \\vdots \\\\\n",
        "a_{n 1} & a_{n 2} & \\ldots & a_{n n}\n",
        "\\end{array}\\right| .\n",
        "$$\n",
        "\n",
        "- How to compute for a $2\\times 2$ matrix?\n",
        "$$\n",
        "\\operatorname{det}(\\boldsymbol{A})=\\left|\\begin{array}{ll}\n",
        "a_{11} & a_{12} \\\\\n",
        "a_{21} & a_{22}\n",
        "\\end{array}\\right|=a_{11} a_{22}-a_{12} a_{21} .\n",
        "$$\n",
        "\n",
        "- E.g., $\\left|\\begin{array}{ll}\n",
        "2 & 1 \\\\\n",
        "0 & 2\n",
        "\\end{array}\\right|=2\\times 2 - 0\\times 1 = 4$\n",
        "\n",
        "- But what's the REAL intuition behind determinant?\\\n",
        "Let's decode it step by step:\n",
        "    1. We all know what matrix transformation is right? <center><img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZvFXLtqy2_iGGs4rHNxhNg.png\"></center>\n",
        "    2. Look more closely, what's it doing: <center><img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zDbJZGxtTi5lqrGebgmseQ.png\"></center>\n",
        "    3. Exactly! It looks like our chosen matrix stretches space apart. Whatever area in the input space we choose, it seems that after the transformation the area gets bigger. This is precisely what the determinant is! ***The determinant of a matrix is the factor by which areas are scaled by this matrix.***\n",
        "    4. Because matrices are linear transformations it is enough to know the scaling factor for one single area to know the scaling factor for all areas. <center><img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QE93C-F8Pa0thOf9Up-BXQ.png\"></center>\n",
        "    5. The rectangle inscribed by the pink and blue unit vectors and has an area of 1. After applying our matrix transformation, this rectangle has turned into a parallelogram with base 2 and height 2. So it has an area of 4. This means, that our matrix scales areas by a factor of 4. Therefore, the determinant of our matrix is 4. NEAT, right?\n",
        "    6. BUT determinants can be **negative** right. How to deal about that? If a matrix has a negative determinant, then it just means that space reversed its orientation. <center><img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*r9UQamEXvdOfAGHB71dLSA.png\"></center>\n",
        "    7. More formally, The determinant is the signed volume of the parallelepiped formed by the columns of the matrix.\n",
        "    8. Now a BIGGGG one: `If a matrix has a determinant of 0 it is non-invertible.` WHY? Any idea?\n",
        "    <!-- This means that the matrix scales all areas by a factor of 0, which in turn means that all areas become 0 after the transformation. This can only happen if the matrix squishes the whole space into a lower dimension. For example, the two-dimensional space would be squished into a single line or point and such a transformation cannot be undone. -->\n",
        "\n",
        "- Properties of Determinant:\n",
        "    1. The determinant of a matrix product is the product of the corresponding  determinants, $\\operatorname{det}(\\boldsymbol{A B})=\\operatorname{det}(\\boldsymbol{A}) \\operatorname{det}(\\boldsymbol{B})$.\n",
        "    2. Determinants are invariant to transposition, i.e., $\\operatorname{det}(\\boldsymbol{A})=\\operatorname{det}\\left(\\boldsymbol{A}^{\\top}\\right)$.\n",
        "    3. If $\\boldsymbol{A}$ is regular (invertible), then $\\operatorname{det}\\left(\\boldsymbol{A}^{-1}\\right)=\\dfrac{1}{\\operatorname{det}(\\boldsymbol{A})}$.\n",
        "    4. Multiplication of a column/row with $\\lambda \\in \\mathbb{R}$ scales $\\operatorname{det}(\\boldsymbol{A})$ by $\\lambda$. In particular, $\\operatorname{det}(\\lambda \\boldsymbol{A})=\\lambda^{n} \\operatorname{det}(\\boldsymbol{A})$.\n",
        "    5. Swapping two rows/columns changes the sign of $\\operatorname{det}(\\boldsymbol{A})$.\n",
        "\n",
        "To know more and gain beautiful intuition, follow this [link](https://www.youtube.com/watch?v=Ip3X9LOh2dk)"
      ],
      "metadata": {
        "id": "WgSsW05eXYtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ip3X9LOh2dk?start=47\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "n5zcEM1FxByx",
        "outputId": "151ea956-b457-4758-c821-5e750f8070e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ip3X9LOh2dk?start=47\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute determinant of matrix...\n",
        "A = np.array([[2,3,5], [4,-2,-7], [9,5,-3]])\n",
        "print(f\"A = \\n{A}\")\n",
        "\n",
        "# Det\n",
        "detA = np.linalg.det(A)\n",
        "print(f\"Det(A) = {detA:.3f}\")\n",
        "\n",
        "# Verifying properties...\n",
        "B = np.array([[1,3,5], [4,-1,-7], [8,5,-3]])\n",
        "print(f\"1. Det(A) = {detA:.3f}, Det(B) = {np.linalg.det(B):.3f}, Det(AB) = {np.linalg.det(A @ B):.3f}\")\n",
        "print(f\"2. Det(A) = {detA:.3f}, Det(A^T) = {np.linalg.det(A.T):.3f}\")\n",
        "print(f\"3. Det(A^-1) = {np.linalg.det(np.linalg.inv(A)):.3f}, 1/Det(A) =  {1/detA:.3f}\")\n",
        "print(f\"4. Det(2*A) = {np.linalg.det(2*A):.3f}, (2^3)*Det(A) = {(2**3)*detA:.3f}\")\n",
        "# Swapping first and second row\n",
        "A[[0,1]] = A[[1,0]]\n",
        "print(f\"5. Det(A) = {detA:.3f}, swapping 1st and 2nd row = {np.linalg.det(A):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEKcQVo8LDUb",
        "outputId": "10ccf787-6917-4d59-f2eb-89f3cc784a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = \n",
            "[[ 2  3  5]\n",
            " [ 4 -2 -7]\n",
            " [ 9  5 -3]]\n",
            "Det(A) = 119.000\n",
            "1. Det(A) = 119.000, Det(B) = 46.000, Det(AB) = 5474.000\n",
            "2. Det(A) = 119.000, Det(A^T) = 119.000\n",
            "3. Det(A^-1) = 0.008, 1/Det(A) =  0.008\n",
            "4. Det(2*A) = 952.000, (2^3)*Det(A) = 952.000\n",
            "5. Det(A) = 119.000, swapping 1st and 2nd row = -119.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [OPTIONAL]: Trace of $\\boldsymbol{A}$\n",
        "\n",
        "**Definition**: The trace of a square matrix $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$ is defined as $$\n",
        "\\operatorname{tr}(\\boldsymbol{A}):=\\sum_{i=1}^{n} a_{i i}\n",
        "$$ i.e. , the trace is the sum of the principal diagonal elements of $\\boldsymbol{A}$.\n",
        "\n",
        "The trace satisfies the following properties:\n",
        "- $\\operatorname{tr}(\\boldsymbol{A}+\\boldsymbol{B})=\\operatorname{tr}(\\boldsymbol{A})+\\operatorname{tr}(\\boldsymbol{B})$ for $\\boldsymbol{A}, \\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$\n",
        "- $\\operatorname{tr}(\\alpha \\boldsymbol{A})=\\alpha \\operatorname{tr}(\\boldsymbol{A}), \\alpha \\in \\mathbb{R}$ for $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$\n",
        "- $\\operatorname{tr}\\left(\\boldsymbol{I}_{n}\\right)=n$\n",
        "- $\\operatorname{tr}(\\boldsymbol{A B})=\\operatorname{tr}(\\boldsymbol{B} \\boldsymbol{A})$ for $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times k}, \\boldsymbol{B} \\in \\mathbb{R}^{k \\times n}$"
      ],
      "metadata": {
        "id": "HTnRQhSKnsEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute trace of Matrix...\n",
        "A = np.array([[2,3,5], [4,-2,-7], [9,5,-3]])\n",
        "print(f\"A = \\n{A}\")\n",
        "B = np.array([[1,3,5], [4,1,-7], [8,5,-3]])\n",
        "print(f\"B = \\n{B}\")\n",
        "\n",
        "# Trace...\n",
        "trA = np.sum(np.diag(A))\n",
        "trB = np.sum(np.diag(B))\n",
        "print(f\"Trace(A) = {trA}\\nTrace(B) = {trB}\")\n",
        "\n",
        "# Verify properties...\n",
        "print(f\"1. Trace(A+B) = {np.trace(A+B)}, and Trace(A) + Trace(B) = {trA + trB}\")\n",
        "print(f\"2. Trace(2*A) = {np.trace(2*A)}, and 2*Trace(A) = {2*trA}\")\n",
        "print(f\"3. Trace(I_3) = {np.trace(np.eye(3))}\")\n",
        "print(f\"4. Trace(AB) = {np.trace(A @ B)}, and Trace(BA) = {np.trace(B @ A)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8nWeM45NtMF",
        "outputId": "27899cb4-4a8d-4e16-e1a3-f90ab6be2dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = \n",
            "[[ 2  3  5]\n",
            " [ 4 -2 -7]\n",
            " [ 9  5 -3]]\n",
            "B = \n",
            "[[ 1  3  5]\n",
            " [ 4  1 -7]\n",
            " [ 8  5 -3]]\n",
            "Trace(A) = -3\n",
            "Trace(B) = -1\n",
            "1. Trace(A+B) = -4, and Trace(A) + Trace(B) = -4\n",
            "2. Trace(2*A) = -6, and 2*Trace(A) = -6\n",
            "3. Trace(I_3) = 3.0\n",
            "4. Trace(AB) = 48, and Trace(BA) = 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eigen-Values and Eigen-Vectors\n",
        "\n",
        "$$\n",
        "\\boldsymbol{A x}=\\lambda \\boldsymbol{x} .\n",
        "$$\n",
        "where,\n",
        "1. $\\lambda$ is an eigenvalue of $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$.\n",
        "2. And $\\boldsymbol{x} \\in \\mathbb{R}^{n} \\backslash\\{\\mathbf{0}\\}$ is such a vector that satisfies $\\boldsymbol{A x}=\\lambda \\boldsymbol{x}$, or equivalently, $(\\boldsymbol{A}$ $\\left.\\lambda \\boldsymbol{I}_{n}\\right) \\boldsymbol{x}=\\mathbf{0}$ can be solved non-trivially, i.e., $\\boldsymbol{x} \\neq \\mathbf{0}$.\n",
        "\n",
        "- First of all what is *eigen*? Eigen is a German word meaning \"characteristic\", \"self\", or \"own\". WHY? such words?\n",
        "<!-- It is because after transformation vector got knocked off from its span. But there are some, who remains in their span or same line (direction can also change) -->"
      ],
      "metadata": {
        "id": "1-cSuP7TtzH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PFDu9oVAE-g?start=81\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "ZlTPhnlswAPZ",
        "outputId": "3bf1820c-09e9-4e6b-84b2-c447d8dc9492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PFDu9oVAE-g?start=81\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to solve for eigen-values and eigen-vectors?**\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x} & \\Longleftrightarrow \\boldsymbol{A} \\boldsymbol{x}-\\lambda \\boldsymbol{x}=\\mathbf{0} \\\\\n",
        "& \\Longleftrightarrow(\\boldsymbol{A}-\\lambda \\boldsymbol{I}) \\boldsymbol{x}=\\mathbf{0}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- Now consider this as a SLE, where $\\lambda$ as well as co-ordinates of $\\boldsymbol{x}$ are still unknown. And we need to solve for them. This is what we popularly call `Characteristic equation`\n",
        "\n",
        "- Let's do one with our code."
      ],
      "metadata": {
        "id": "JEnLdomvxh7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute eigen-values and eigen-vectors (space = R^3)\n",
        "# Note: We'll throughout this session will deal with real spaces ONLY.\n",
        "A = np.array([[-1, -2, 2], [4,3,-4], [0,-2,1]])\n",
        "print(f\"A = \\n{A}\")\n",
        "\n",
        "# Compute eigen values and vectors\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "print(f\"Eigen-values of A:\\n{eigenvalues}\")\n",
        "print(f\"Eigen-vectors of A:\\n{eigenvectors}\")\n",
        "\n",
        "# Verify:\n",
        "print(f\"A*v1 = \\n{A @ eigenvectors[:,0:1]},\\nand 3*v1 = \\n{3*eigenvectors[:,0:1]}\\n\\n\")\n",
        "print(f\"A*v2 = \\n{A @ eigenvectors[:,1:2]},\\nand 1*v2 = \\n{1*eigenvectors[:,1:2]}\\n\\n\")\n",
        "print(f\"A*v3 = \\n{A @ eigenvectors[:,2:3]},\\nand -1*v3 = \\n{-1*eigenvectors[:,2:3]}\\n\\n\")"
      ],
      "metadata": {
        "id": "dnGIDyER_cEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982db4b6-0b83-4edf-81d8-7c039cb65c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = \n",
            "[[-1 -2  2]\n",
            " [ 4  3 -4]\n",
            " [ 0 -2  1]]\n",
            "Eigen-values of A:\n",
            "[ 3.  1. -1.]\n",
            "Eigen-vectors of A:\n",
            "[[-5.77350269e-01 -7.07106781e-01  0.00000000e+00]\n",
            " [ 5.77350269e-01 -2.56395025e-16  7.07106781e-01]\n",
            " [-5.77350269e-01 -7.07106781e-01  7.07106781e-01]]\n",
            "A*v1 = \n",
            "[[-1.73205081]\n",
            " [ 1.73205081]\n",
            " [-1.73205081]],\n",
            "and 3*v1 = \n",
            "[[-1.73205081]\n",
            " [ 1.73205081]\n",
            " [-1.73205081]]\n",
            "\n",
            "\n",
            "A*v2 = \n",
            "[[-7.07106781e-01]\n",
            " [-8.88178420e-16]\n",
            " [-7.07106781e-01]],\n",
            "and 1*v2 = \n",
            "[[-7.07106781e-01]\n",
            " [-2.56395025e-16]\n",
            " [-7.07106781e-01]]\n",
            "\n",
            "\n",
            "A*v3 = \n",
            "[[-4.44089210e-16]\n",
            " [-7.07106781e-01]\n",
            " [-7.07106781e-01]],\n",
            "and -1*v3 = \n",
            "[[-0.        ]\n",
            " [-0.70710678]\n",
            " [-0.70710678]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability and Statistics\n",
        "\n",
        "Contents:\n",
        "1. Joint probability\n",
        "2. Conditional probability\n",
        "3. Marginal probability"
      ],
      "metadata": {
        "id": "xnvmIXEV10wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joint Distribution: $P(X, Y)$\n",
        "\n",
        "- Consider the discrete joint probability below (satisfies the total sum $=1$)\n",
        "\n",
        "| P(X,Y) \t| Y = 1 \t| Y = 2 \t| Y = 3 \t|      \t|\n",
        "|:------:\t|:-----:\t|:-----:\t|:-----:\t|:----:\t|\n",
        "|  X = 1 \t|  1/6  \t|  1/6  \t|   0   \t|  1/3 \t|\n",
        "|  X = 2 \t|   0   \t|  1/6  \t|  1/4  \t| 5/12 \t|\n",
        "|  X = 3 \t|  1/12 \t|   0   \t|  1/6  \t|  1/4 \t|\n",
        "|       \t|  1/4  \t|  1/3  \t|  5/12 \t|   1  \t|\n",
        "\n",
        "- When both $X$ and $Y$, combinedly follows *Gaussian* distribution. <center><img src=\"https://drive.google.com/uc?export=view&id=1jo1Pq52orFeMBmMXmre6RaVHHI2SuDo9\" width=\"400\"></center>\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1SWNWacOP9uvHfaDGCSEejhBE2pQmrFxB\" width=\"400\"></center>\n",
        "\n",
        "## Marginal Distribution: $P(X)$ or $P(Y)$\n",
        "\n",
        "|  P(X,Y)  \t|  Y = 1  \t|  Y = 2  \t|   Y = 3  \t| **P(X)** \t|\n",
        "|:--------:\t|:-------:\t|:-------:\t|:--------:\t|:--------:\t|\n",
        "|   X = 1  \t|   1/6   \t|   1/6   \t|     0    \t|  **1/3** \t|\n",
        "|   X = 2  \t|    0    \t|   1/6   \t|    1/4   \t| **5/12** \t|\n",
        "|   X = 3  \t|   1/12  \t|    0    \t|    1/6   \t|  **1/4** \t|\n",
        "| **P(Y)** \t| **1/4** \t| **1/3** \t| **5/12** \t|   **1**  \t|\n",
        "\n",
        "- Why margin? because they appear at the margin!😸\n",
        "- In terms of Joint Gaussian distribution, the projection of joint-distribution on x-z plane and y-z plane.\n",
        "- In terms of Venn Diagrams: $Area(A)$ and $Area(B)$.\n",
        "\n",
        "## Conditional Distribution: $P(X|Y)$ or $P(Y|X)$\n",
        "- In terms of discrete distribution: What is $P(X|Y = 3)$?\n",
        "\n",
        "|  P(X,Y)  \t     |   Y = 3  \t|\n",
        "|:-------------: |:--------:\t|\n",
        "|   P(X = 1, Y=3)|     0    \t|\n",
        "|   P(X = 2, Y=3)|    1/4   \t|\n",
        "|   P(X = 3, Y=3)|    1/6   \t|\n",
        "| P(Y = 3)       | **5/12** \t|\n",
        "\n",
        "> We haven't answered the above question yet. After next illustration will do\n",
        "\n",
        "- In terms of Venn, <center><img src=\"https://drive.google.com/uc?export=view&id=1lSGMslKAphvdFU6E5jptJBPHjEGv_y-h\" width=\"400\"></center>\n",
        "\n",
        "- Now, $P(X|Y = 3)$ is as follows which sums up to $1$.\n",
        "\n",
        "|  P(X,Y)  \t     |   Y = 3  \t|\n",
        "|:-------------: |:--------:\t|\n",
        "|   P(X = 1, Y=3)|     0    \t|\n",
        "|   P(X = 2, Y=3)|    3/5   \t|\n",
        "|   P(X = 3, Y=3)|    2/5   \t|\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fZzTrbD55ZEb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGk0vCgzFtXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Resources:\n",
        "\n",
        "1. https://bvanderlei.github.io/jupyter-guide-to-linear-algebra/\n",
        "2. https://notebook.community/dcavar/python-tutorial-for-ipython/notebooks/Linear%20Algebra\n",
        "3. http://rlhick.people.wm.edu/stories/linear-algebra-python-basics.html\n",
        "4. https://colab.research.google.com/github/jonkrohn/ML-foundations/blob/master/notebooks/1-intro-to-linear-algebra.ipynb\n",
        "5. https://pythonnumericalmethods.berkeley.edu/notebooks/chapter14.01-Basics-of-Linear-Algebra.html"
      ],
      "metadata": {
        "id": "TvOq7159vltO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # ## Linear Algebra\n",
        "# - System of Linear equations\n",
        "# - Formulting matrix-vector product and solve for them...\n",
        "# - connecting it to intuition of line intersection and how matrix transforms one vector to another.\n",
        "# - Linear independence intuition - via - 2D plane and two vectors.\n",
        "# - Inner products - interms of geometry\n",
        "# - Norms - originate from inner products.\n",
        "# - Angles -> Cosine similarity\n",
        "# - Orthogonality -> Least pojection\n",
        "# - Determinant - Physical intuition\n",
        "# - Traces - \"\n",
        "# - Lastly - What are eigenvalues and eigenvectors - Via intuition\n",
        "\n",
        "# ## Prob and Stats\n",
        "# - What are conditional, marginal, joint. -->"
      ],
      "metadata": {
        "id": "z2F0C-plxfrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- [link text](https://)# **Introduction to Python** -->"
      ],
      "metadata": {
        "id": "ooXcgi5fjJIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Python is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable.\n",
        "\n",
        "\n",
        "Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing. -->"
      ],
      "metadata": {
        "id": "6IH-u381jMun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- In this tutorial, we will cover:\n",
        "\n",
        "* Basic data types\n",
        "* Conditional and Control statements\n",
        "* Containers (Lists, Dictionaries, Sets, Tuples)\n",
        "* Functions, Classes -->\n"
      ],
      "metadata": {
        "id": "FFN03u9bjPBq"
      }
    }
  ]
}